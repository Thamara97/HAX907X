---
title: "TP n°2 : Arbres"
format:
  pdf:
    toc: false
    number-sections: true
    colorlinks: true
    code-fold: true
jupyter: python3
---

Commençons par importer tous les modules nécessaires à la réalisation de ce TP.

```{python}
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rc
import graphviz

from sklearn import tree, datasets
from sklearn.model_selection import train_test_split
from tp_arbres_source import (rand_gauss, rand_bi_gauss, rand_tri_gauss,
                              rand_checkers, rand_clown,
                              plot_2d, frontiere)
```

```{python}
#|echo: False

rc('font', **{'family': 'sans-serif', 'sans-serif': ['Computer Modern Roman']})
params = {'axes.labelsize': 6,
          'font.size': 12,
          'legend.fontsize': 12,
          'text.usetex': False,
          'figure.figsize': (10, 12)}
plt.rcParams.update(params)

sns.set_context("poster")
sns.set_palette("colorblind")
sns.set_style("white")
_ = sns.axes_style()
```

# Classification avec les arbres

## Question 1

Dans le cadre de la régression une mesure d'homogénétié possible serait la variance de l'ensemble $R \in \mathbb{R}^p$ :
$$
\frac{1}{|R|} \sum_{i | x_i \in R} (y_i - \bar y_R)^2
$$

où $\bar y_R = \frac{1}{|R|} \sum_{i | x_i \in R} y_i$.

En effet, cette mesure nous permettrait d'avoir des ensembles avec une variance minimale donc des ensembles homogènes.

## Question 2

Simulons des échantillons de taille $n=456$ avec la fonction `rand_checkers`.

```{python}
np.random.seed(1)

n1 = 114
n2 = 114
n3 = 114
n4 = 114
data = rand_checkers(n1, n2, n3, n4)
```

Nos données sont réparties en quatre classes et nous les représentons graphiquement en @fig-data-checkers.

```{python}
#| label: fig-data-checkers
#| fig-cap: Jeu de données
plt.ion()
plt.title('Data set')
plot_2d(data[:, :2], data[:, 2], w=None)
```

Crééons deux courbes, une pour l'indice Gini (en rouget) et une pour l'entropie (en vert), qui donnent le pourcentage d'erreurs commises en fonction de la profondeur maximale de l'arbre.

```{python}
#| fig-cap: Pourcentage d'erreurs commises sur les données d'apprentissage
#| label: fig-erreur-train
X_train = data[:, :2]
Y_train = data[:, 2].astype(int)

dmax = 12
error_entropy = np.zeros(dmax)
error_gini = np.zeros(dmax)

for i in range(dmax):
    dt_entropy = tree.DecisionTreeClassifier(criterion = "entropy", max_depth = i + 1)
    dt_entropy.fit(X_train, Y_train)
    error_entropy[i] = 1 - dt_entropy.score(X_train, Y_train)

    dt_gini = tree.DecisionTreeClassifier(criterion = "gini", max_depth = i + 1)
    dt_gini.fit(X_train, Y_train)
    error_gini[i] = 1 - dt_gini.score(X_train, Y_train)

plt.figure()
plt.plot(error_entropy * 100, 'g')
plt.plot(error_gini * 100, 'r')
plt.xlabel('Profondeur maximale')
plt.ylabel('Pourcentage d\'erreur')
plt.draw()
```

Nous constatons (@fig-erreur-train) que pour les deux critères, le pourcentage d'erreurs diminue quand la profondeur de l'arbre augmente, il tend vers 0. Cela ne signifie pas pour autant qu'un arbre avec un profondeur élevée permettra une meilleure prédiction. 

Notons que nous calculons l'erreur à partir des données d'apprentissage. Nous sommes donc dans un cas de surapprentissage, où l'arbre construit est totalement adapté à la prédiction des données d'aprentissage, ce ne sera probablement pas le cas pour de nouvelles données.

Remarquons également que bien que les mesures d'impureté utilisées soient différentes, les pourcentages d'erreurs sont proches pour des pronfondeurs très petite sou très grandes. Dans le premier cas le porucentage d'erreurs est nécessairement grand (environ 70% ici), tandis que dans le second il est proche de 0.

## Question 3

Affichons la classification obtenue en utilisant la profondeur de l'arbre qui minimise le pourcentage d'erreurs obtenues avec l'entropie (@fig-calssif).

```{python}
#| label: fig-calssif
#| fig-cap: Meilleure classification des données avec l'entropie
dt_entropy.max_depth = np.where(error_entropy == min(error_entropy))[0][0] + 1
dt_entropy.fit(X_train, Y_train)

plt.figure()
frontiere(lambda x: dt_entropy.predict(x.reshape((1, -1))), X_train, Y_train, step=100)
plt.title("Best frontier with entropy criterion")
plt.draw()
```

La majorité des données d'apprentissage sont assez bien classée. Cependant les frontières obtenus sont assez complexes puisqu'une profondeur maximale élevée signifie qu'on autorise une découpe assez fini de l'espace.

## Question 4

Nous exportons un graphique de l'arbre obtenu à la question précédente au format pdf. Pour cela nous utilisons la fonction `export-graphiz`du module `tree`.

```{python}
#| output: false
dot_data = tree.export_graphviz(dt_entropy, out_file=None)
graph = graphviz.Source(dot_data)
graph.render("Arbre")
```

Nous obtenons une représentation de notre arbre de décision. Pour chaque nœud nous avons :

* son entropie
* le nombre d'observations
* le nombre d'observations par classe 
* la variable et le seuil utilisés pour partitionner l'ensemble des observations du nœud

## Question 5

Créons $160$ nouvelles données avec `rand_checkers`. Elle nous servirons d'échantillon de test.

```{python}
data_test = rand_checkers(40, 40, 40, 40)
X_test = data_test[:, :2]
Y_test = data_test[:, 2].astype(int)
```

Maintenant, nous reprenons les arbres de décision entraînés précédemment et nous calculons la proportion d'erreurs faites sur cet échantillon de test.

```{python}
dmax = 12
error_entropy = np.zeros(dmax)
error_gini = np.zeros(dmax)

for i in range(dmax):
    dt_entropy = tree.DecisionTreeClassifier(criterion = "entropy", max_depth = i + 1)
    dt_entropy.fit(X_train, Y_train)
    error_entropy[i] = 1 - dt_entropy.score(X_test, Y_test)

    dt_gini = tree.DecisionTreeClassifier(criterion = "gini", max_depth = i + 1)
    dt_gini.fit(X_train, Y_train)
    error_gini[i] = 1 - dt_gini.score(X_test, Y_test)
```

Nous obtenons les courbes représentées en @fig-erreur-test.

```{python}
#| echo: false
#| label: fig-erreur-test
#| fig-cap: Pourcentage d'erreurs commises sur les données de test

plt.figure()
plt.plot(error_entropy * 100, 'g')
plt.plot(error_gini * 100, 'r')
plt.xlabel('Profondeur maximale')
plt.ylabel('Pourcentage d\'erreur')
plt.draw()
```

La courbe verte est obtenue en utilisant l'entropie comme mesure d'impureté et la rouge en utilisant l'indicie Gini.

Comme précédemment le pourcentage d'erreurs diminu à mesure que la profondeur maximale de l'arbre augemente. Cependant, le minimum est supérieur de 10%, tandis que pour les données d'entraînement il était proche de 0%. Ce pourcentage semble même augmenter quand la profondeur maximale devient trop importante avec l'indice Gini.

Un arbre très profond, ne fournit pas nécessairement une meilleure prédiction.

## Question 6

Nous allons maintenant utiliser les données du dataset DIGITS disponible dans le module `sklearn.datasets`.

```{python}
digits = datasets.load_digits()
```

Il s'agit d'un ensemble d'images de 8x8 pixels. Nous en représentons 4 en @fig-exemple-digits.

```{python}
#| label: fig-exemple-digits
#| fig-cap: Exemple de données

_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))
for ax, image, label in zip(axes, digits.images, digits.target):
    ax.set_axis_off()
    ax.imshow(image, cmap=plt.cm.gray_r, interpolation="nearest")
    ax.set_title("Training: %i" % label)
```

Nous coupons notre échantillons en deux, 80% des données seront des données de test et les 20% restant seront les données d'apprentissage.

```{python}
n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))

X_train, X_test, Y_train, Y_test = train_test_split(data, digits.target, test_size = 0.8)
```

Nous pouvons maintenant représenter les courbes qui donnent le pourcentage d'erreurs commises en fonction de la profondeur maximale de l'arbre pour les deux mesures d'impureté (l'indice Gini en rouge et l'entropie en vert). (@fig-erreur-digits-train)

```{python}
#| echo: false
#| label: fig-erreur-digits-train
#| fig-cap: Pourcentage d'erreurs commises sur les données d'entraînement

dmax = 12
error_entropy = np.zeros(dmax)
error_gini = np.zeros(dmax)

for i in range(dmax):
    dt_entropy = tree.DecisionTreeClassifier(criterion = "entropy", max_depth = i + 1)
    dt_entropy.fit(X_train, Y_train)
    error_entropy[i] = 1 - dt_entropy.score(X_train, Y_train)

    dt_gini = tree.DecisionTreeClassifier(criterion = "gini", max_depth = i + 1)
    dt_gini.fit(X_train, Y_train)
    error_gini[i] = 1 - dt_gini.score(X_train, Y_train)

plt.figure()
plt.plot(error_entropy * 100, 'g')
plt.plot(error_gini * 100, 'r')
plt.xlabel('Profondeur maximale')
plt.ylabel('Pourcentage d\'erreur')
plt.draw()
```

Là encore, le pourcentage d'erreurs commises sur les données d'apprentissage décroit quand la profondeur de l'arbre croit. Il atteind 0% à partir d'une certaine profondeur dans les deux cas.

Nous visualisons en @fig-predict-digits-train 4 observations faisant partie de l'échantillon d'apprentissage et la prédiction obtenue avec l'arbre de décision qui minimise le pourcentage d'erreurs.

```{python}
#| echo: false
#| label: fig-predict-digits-train
#| fig-cap: Exemple de prédiction des données d'apprentissage

dt_entropy.max_depth = np.where(error_entropy == min(error_entropy))[0][0] + 1
dt_entropy.fit(X_train, Y_train)
predict = dt_entropy.predict(X_train)

_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))
for ax, image, prediction in zip(axes, X_train, predict):
    ax.set_axis_off()
    image = image.reshape(8, 8)
    ax.imshow(image, cmap=plt.cm.gray_r, interpolation="nearest")
    ax.set_title(f"Prediction: {prediction}")
```

Les 4 images sont correctement prédites, ce qui est normal puisqu'à la profondeur de l'arbre choisie, le pourcentage d'erreurs commises est nul.

Nous exportons comme précédemment un graphique de l'arbre obtenu.

```{python}
#| output: false
dot_data = tree.export_graphviz(dt_entropy, out_file=None)
graph = graphviz.Source(dot_data)
graph.render("Arbre_digits")
```

Enfin nous calculons la proportion d'erreurs faites par les arbres de décisions entraînés précédemment sur l'échantillon de test (@fig-erreur-digits-test).

```{python}
#| echo: false
#| label: fig-erreur-digits-test
#| fig-cap: Pourcentage d'erreurs commises sur les données de test

dmax = 12
error_entropy = np.zeros(dmax)
error_gini = np.zeros(dmax)

for i in range(dmax):
    dt_entropy = tree.DecisionTreeClassifier(criterion = "entropy", max_depth = i + 1)
    dt_entropy.fit(X_train, Y_train)
    error_entropy[i] = 1 - dt_entropy.score(X_test, Y_test)

    dt_gini = tree.DecisionTreeClassifier(criterion = "gini", max_depth = i + 1)
    dt_gini.fit(X_train, Y_train)
    error_gini[i] = 1 - dt_gini.score(X_test, Y_test)

plt.figure()
plt.plot(error_entropy * 100, 'g')
plt.plot(error_gini * 100, 'r')
plt.xlabel('Profondeur maximale')
plt.ylabel('Pourcentage d\'erreur')
plt.draw()
```

Nous constatons qu'avec l'échantillon de test, le pourcentage d'erreurs décroit jusqu'à une certaine profondeur. Cette fois il ne semble pas tendre vers, il y a toujours des erreurs de prédiction.

# Méthodes de choix de paramètres - Sélection de modèle

## Question 7


